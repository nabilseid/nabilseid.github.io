{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Hello , I am Nabil Seid, a senior data engineer. This is my personal page where I share about my personal projects and their documentations, notes about various things I learnt, my collection of quick fixes, boring write ups, and I casually blog about my solution to challenges I faced on work or my personal projects.</p> <p></p> <ul> <li> <p> Projects</p> <ul> <li>athenaSQL</li> </ul> </li> <li> <p> Notes</p> <ul> <li>Grokking Algorithms</li> <li>Big Data Analysis in SQL</li> </ul> </li> <li> <p> Quick Fixes</p> <ul> <li>Bash</li> <li>Python</li> </ul> </li> <li> <p> Blogs</p> <ul> <li>My terminal duo Neovim &amp; Tmux</li> </ul> </li> </ul>"},{"location":"resume/","title":"Resume","text":"<p>View Resume PDF</p> <p></p>"},{"location":"athenaSQL/","title":"athenaSQL","text":"<p>athenaSQL is Athena SQL query builder, inspired by sparkSQL. It borrow some sparkSQL\u2019s concept sparkSQL.</p> <p>It was initially designed to eliminate the need for hard-coding SQL queries as strings within Python scripts and as an alternative to any bespoke SQL query templating. However, it offers the flexibility to be used in various ways as needed.</p>"},{"location":"athenaSQL/#installing-athenasql","title":"Installing athenaSQL","text":"<pre><code>$ pip install athenaSQL\n</code></pre>"},{"location":"athenaSQL/#usage","title":"Usage","text":"<p>Using athenaSQL is stright forward. First we create a table abstraction class then building a query is just calling chain methods on top of it.</p> <pre><code>from athenaSQL import Athena\n\n# creating athena table instance from database\ntable = Athena('database_name').table('table_name')\n\n# creating athena table instance from database\nquery = table.select()\n\nquery.show_query()\n</code></pre> <pre><code>SELECT\n    *\nFROM \"database_name\".\"table_name\"\n</code></pre>"},{"location":"athenaSQL/columns/","title":"Columns","text":"<p>Column type is used to represent a single column. It is constracted just by passing the column name as an argument. Column type are needed to support python built-in operation, to identify what type of column has passed and to constract column level queries like window functions and aliasing.</p> <p>There are different kinds of columns. We are only gonna use two of them. There rest are constracted behind the scene. A plane column and new column.</p> <p>Plane column is a column type with only a column name, we need this when we want to do comparison, arithmetic and logical operations on a column otherwise we can use the column name instead. We can use either <code>col()</code> or <code>column()</code> functions to create plane column.</p> <pre><code>import athenaSQL.functions as F\n\nsalary_table.select('city',                    # using column name\n                    F.col('country'),          # using col()\n                    F.max('salary_avg'),       # passing column name to function\n                    F.max(F.col('salary_avg')) # passing column to function\n                ).show_query()\n</code></pre> <pre><code>SELECT\n    city,\n    country,\n    MAX(salary_avg),\n    MAX(salary_avg)\nFROM \"employee_db\".\"salary\"\n</code></pre>"},{"location":"athenaSQL/columns/#column-with-data-type","title":"Column with Data Type","text":"<p>New column is used when creating an EXTERNAL table. It requires both column name and data type to create new column. We can use <code>nCol()</code> function to create new column.</p> <p>Data types are imported from <code>athenaSQL.column_type</code>. Supported data types are</p> <ul> <li><code>boolean</code></li> <li><code>tinyint</code></li> <li><code>smallint</code></li> <li><code>int</code></li> <li><code>bigint</code></li> <li><code>double</code></li> <li><code>float</code></li> <li><code>string</code></li> <li><code>binary</code></li> <li><code>date</code></li> <li><code>timestamp</code></li> <li><code>char [ (length) ]</code></li> <li><code>varchar [ (length) ]</code></li> <li><code>decimal [ (precision, scale) ]</code></li> </ul> <p>More reference</p> <pre><code>import athenaSQL.functions as F\nfrom athenaSQL.column_type import COLUMNTYPE\n\nproduct_table = AthenaTable(database='product_db', table='product')\n\n(product_table.create()\n    .columns(\n        F.nCol('product_id', COLUMNTYPE.int),\n        F.nCol('product_name', COLUMNTYPE.varchar(50)),\n        F.nCol('product_detail', COLUMNTYPE.string))\n    .show_query()\n)\n</code></pre> <pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS\n\"product_db\".\"product\" (\n    `product_id` int,\n    `product_name` varchar(50),\n    `product_detail` string\n)\n</code></pre>"},{"location":"athenaSQL/columns/#column-operations","title":"Column Operations","text":"<p>Comparison, arithmetic and logical operations can be done using python\u2019s built-in operators on a column. If column is string it should be wrapped  with a column type.</p> <p>Supported operations</p> <ul> <li>Comparison Operations: <code>&lt;</code>, <code>&lt;=</code>, <code>=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>!=</code></li> <li>Arithmetic Operations: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code>, <code>pow()</code>, <code>abs()</code></li> <li>Logical Operations: <code>&amp;</code>, <code>|</code>, <code>~</code></li> </ul> <p>All logical operations should be enclosed by parenthesis. Parenthesis has a higher precedence than all supported operations.</p> <p>Reverese operation is supported on all comparison, all arithmetic except <code>abs()</code> and all logical except <code>~</code>.</p>"},{"location":"athenaSQL/columns/#comparison-operations","title":"Comparison Operations","text":"<pre><code>print(F.col('age') &gt;= 10)\nprint(F.col('lang') !=  'python')\nprint(F.max('age') &lt; 50)\nprint(50 &lt; F.max('age')) # reverse '&lt;'\n</code></pre> <pre><code>age &gt;= 10\nlang &lt;&gt; 'python'\nMAX(age) &lt; 50\nMAX(age) &gt; 50 -- reverse '&lt;'\n</code></pre>"},{"location":"athenaSQL/columns/#arithmetic-operations","title":"Arithmetic Operations","text":"<pre><code>print(F.col('revenue') - F.col('cost'))\nprint((F.col('revenue') /  5).alias('rev_5'))\nprint(pow(F.max('age'), 4))\nprint(abs(F.col('cost') - 5000))\nprint((1 - F.col('profite_fraction')).alias('cost_fraction')) # reverse '-'\n</code></pre> <pre><code>revenue - cost\nrevenue / 5 AS rev_5\nPOWER(MAX(age), 4)\nABS(cost - 5000)\n1 - profite_fraction AS cost_fraction -- reverse '-'\n</code></pre>"},{"location":"athenaSQL/columns/#logical-operations","title":"Logical Operations","text":"<pre><code>print(~(F.col('lang') ==  'python'))\nprint((F.col('age') &gt; 10) &amp; (F.col('age') &lt; 20))\nprint(((F.col('age') &gt; 10) &amp; (F.col('age') &lt; 20)) | ~(F.col('is_infant') == True))\n</code></pre> <pre><code>NOT(lang = 'python')\n(age &gt; 10 AND age &lt; 20)\n((age &gt; 10 AND age &lt; 20) OR NOT(is_infant = True))\n</code></pre> <p>In addition to the column types mentioned there are other types which are used to identify different columns.</p> <ul> <li><code>CaseColumn</code>: return type from a case function</li> <li><code>AliasColumn</code>: an aliased column</li> <li><code>WindowColumn</code>: a column with window function</li> <li><code>AggregateColumn</code>: a column wrapped with an aggregation function</li> <li><code>FunctionalColumn</code>: a column wrapped with a function</li> <li><code>ConditionalColumn</code>: column with a logical operation</li> </ul>"},{"location":"athenaSQL/concept/","title":"Concept","text":"<p>athenaSQL is Athena SQL query builder, inspired by sparkSQL. It has 4 main components: </p> <ul> <li>Database &amp; Tables: abstraction for databases and tables.</li> <li>Columns: abstraction for table columns.</li> <li>Queries: queries that are performed on top of a table.</li> <li>Functions: column level SQL functions </li> </ul> <p>TODO typs: <code>Column type</code>, <code>Order type</code></p>"},{"location":"athenaSQL/databases_and_tables/","title":"Databases &amp; Tables","text":"<p>athenaSQL uses <code>Athena</code> class to abstracted athena databases and <code>AthenaTable</code> for athena tables. CTE temporary tables are abstracted by <code>TempTable</code>.</p> <p><code>AthenaTable</code> cannot be constracted without a database. <code>TempTable</code> on the other hand doesn\u2019t have database.</p> <p>Note</p> <p>All queries are performed on <code>AthenaTable</code> instance.</p> <p>Only <code>SELECT</code> query is availabile for CTE table. To use CTE with other queries pass it as a select query argument.</p>"},{"location":"athenaSQL/databases_and_tables/#using-athenatable","title":"Using AthenaTable","text":"<pre><code>from athenaSQL import Athena, AthenaTable, TempTable\n\n# creating athena table instance from database\ntable = Athena('db_name').table('table_name')\n\n# creating athena table instance directly\ntable = AthenaTable('db_name', 'table_name')\n\n# creating temp table instance\ntemp_table = TempTable('temp_tbl_name')\n</code></pre>"},{"location":"athenaSQL/functions/","title":"Functions","text":"<p>Functions are performed on columns. You can get more references here.</p> <p>Aggregating functions should be used on non grouping cols if query is grouped. Aggregating functions can also be used in place of window functions.</p> <p><code>sqrt(col)</code>: Computes the square root of the specified float value.</p> <p><code>abs(col)</code>: Computes the absolute value.</p> <p><code>mean(col)</code>: Aggregate function: returns the average of the values in a group.</p> <p><code>geometric_mean(col)</code>: Returns the geometric mean of all input values.</p> <p><code>stddev(col)</code>: Returns the sample standard deviation of all input values.</p> <p><code>variance(col)</code>: Returns the sample variance of all input values.</p>"},{"location":"athenaSQL/functions/#supported-queries","title":"Supported queries","text":"<ul> <li>Aggregation</li> <li>String</li> <li>Date and Time</li> <li>URL</li> <li>Window</li> <li>Unicode</li> <li>UUID</li> </ul>"},{"location":"athenaSQL/functions/aggregation/","title":"Aggregation Functions","text":"<p><code>any_value(col)</code>: Returns an arbitrary non-null value x, if one exists.</p> <p><code>arbitrary(col)</code>: Returns an arbitrary non-null value of x, if one exists. Identical to <code>any_value()</code>.</p> <p><code>array_agg(col)</code>: Returns an array created from the input x elements.</p> <p><code>avg(col)</code>: Returns the average (arithmetic mean) of all input values.</p> <p><code>bool_and(col)</code>: Returns TRUE if every input value is TRUE, otherwise FALSE.</p> <p><code>bool_or(col)</code>: Returns TRUE if any input value is TRUE, otherwise FALSE.</p> <p><code>checksum(col)</code>: Returns an order-insensitive checksum of the given values.</p> <p><code>count(col)</code>: Returns the number of non-null input values.</p> <p><code>count_if(col)</code>: Returns the number of TRUE input values. This function is equivalent to count(CASE WHEN x THEN 1 END).</p> <p><code>every(col)</code>: This is an alias for <code>bool_and()</code>.</p> <p><code>geometric_mean(col)</code>: Returns the geometric mean of all input values.</p> <p><code>max(col)</code>: Returns the maximum value of all input values.</p> <p><code>min(col)</code>: Returns the minimum value of all input values.</p> <p><code>sum(col)</code>: Returns the sum of all input values.</p>"},{"location":"athenaSQL/functions/date_time/","title":"Date and Time Functions","text":"<p><code>date(col)</code>: This is an alias for CAST(x AS date).</p> <p><code>last_day_of_month(col)</code>: Returns the last day of the month.</p> <p><code>from_iso8601_timestamp(col)</code>: Parses the ISO 8601 formatted string into a timestamp with time zone.</p>"},{"location":"athenaSQL/functions/date_time/#date-and-time-convenience-extraction","title":"Date and Time Convenience Extraction","text":"<p><code>day(col)</code>: Returns the day of the month from x.</p> <p><code>day_of_month(col)</code>: This is an alias for day().</p> <p><code>day_of_week(col)</code>: Returns the ISO day of the week from x. The value ranges from 1 (Monday) to 7 (Sunday).</p> <p><code>day_of_year(col)</code>: Returns the day of the year from x. The value ranges from 1 to 366.</p> <p><code>dow(col)</code>: This is an alias for day_of_week().</p> <p><code>doy(col)</code>: This is an alias for day_of_year().</p> <p><code>hour(col)</code>: Returns the hour of the day from x. The value ranges from 0 to 23.</p> <p><code>millisecond(col)</code>: Returns the millisecond of the second from x.</p> <p><code>minute(col)</code>: Returns the minute of the hour from x.</p> <p><code>month(col)</code>: Returns the month of the year from x.</p> <p><code>quarter(col)</code>: Returns the quarter of the year from x. The value ranges from 1 to 4.</p> <p><code>second(col)</code>: Returns the second of the minute from x.</p> <p><code>timezone_hour(col)</code>: Returns the hour of the time zone offset from timestamp.</p> <p><code>timezone_minute(col)</code>: Returns the minute of the time zone offset from timestamp.</p> <p><code>week(col)</code>: Returns the ISO week of the year from x. The value ranges from 1 to 53.</p> <p><code>week_of_year(col)</code>: This is an alias for week().</p> <p><code>year(col)</code>: Returns the year from x.</p> <p><code>year_of_week(col)</code>: Returns the year of the ISO week from x.</p> <p><code>yow(col)</code>: This is an alias for year_of_week().</p>"},{"location":"athenaSQL/functions/string/","title":"String Functions","text":"<p><code>chr(col)</code>: Returns the Unicode code point n as a single character string.</p> <p><code>codepoint(col)</code>: Returns the Unicode code point of the only character of string.</p> <p><code>length(col)</code>: Returns the length of string in characters.</p> <p><code>upper(col)</code>: Converts a string expression to upper case.</p> <p><code>lower(col)</code>: Converts a string expression to upper case.</p> <p><code>ltrim(col)</code>: Removes leading whitespace from string.</p> <p><code>reverse(col)</code>: Returns string with the characters in reverse order.</p> <p><code>rtrim(col)</code>: Removes trailing whitespace from string.</p> <p><code>trim(col)</code>: Removes leading and trailing whitespace from string.</p> <p><code>word_stem(col)</code>: Returns the stem of word in the English language.</p>"},{"location":"athenaSQL/functions/unicode/","title":"Unicode Functions","text":"<p><code>normalize(col)</code>: Transforms string with NFC normalization form.</p> <p><code>to_utf8(col)</code>: Encodes string into a UTF-8 varbinary representation.</p> <p><code>from_utf8(col)</code>: Decodes a UTF-8 encoded string from binary. Invalid UTF-8 sequences are replaced with the Unicode replacement character U+FFFD.</p>"},{"location":"athenaSQL/functions/url/","title":"URL Functions","text":"<p>[protocol:]//host[:port]][path][?query][#fragment]</p> <p><code>url_extract_fragment(col)</code>: Returns the fragment identifier from url.</p> <p><code>url_extract_host(col)</code>: Returns the host from url.</p> <p><code>url_extract_path(col)</code>: Returns the path from url.</p> <p><code>url_extract_port(col)</code>: Returns the port number from url.</p> <p><code>url_extract_protocol(col)</code>: Returns the protocol from url.</p> <p><code>url_extract_query(col)</code>: Returns the query string from url.</p> <p><code>url_encode(col)</code>:</p> <p><code>url_decode(col)</code>: Unescapes the URL encoded value. This function is the inverse of url_encode().</p>"},{"location":"athenaSQL/functions/uuid/","title":"UUID Functions","text":"<p><code>uuid(col)</code>: Returns a pseudo randomly generated UUID (type 4).</p>"},{"location":"athenaSQL/functions/window/","title":"Window Functions","text":"<p><code>cume_dist()</code>: Returns the cumulative distribution of a value in a group of values.</p> <p><code>dense_rank()</code>: Returns the rank of a value in a group of values.</p> <p><code>percent_rank()</code>: Returns the percentage ranking of a value in group of values.</p> <p><code>rank()</code>: Returns the rank of a value in a group of values.</p> <p><code>row_number()</code>: Returns a unique sequential number for each row.</p> <p><code>ntile(n)</code>: Divides the rows for each window partition into n buckets ranging from 1 to at most n.</p> <p><code>first_value(x)</code>: Returns the first value of the window.</p> <p><code>last_value(x)</code>: Returns the last value of the window.</p> <p><code>nth_value(x, offset)</code>: Returns the value at the specified offset from the beginning of the window.</p> <p><code>lead(x, [offset, [default_value]])</code>: Returns the value at offset rows after the current row in the window partition.</p> <p><code>lag(x, [offset, [default_value]])</code>: Returns the value at offset rows before the current row in the window partition.</p>"},{"location":"athenaSQL/queries/","title":"Queries","text":"<p>All queries need either <code>AthenaTable</code> or <code>TempTable</code> to execute. Query  instance has <code>show_query()</code> that let's you preview the sql query constracted  and <code>exec()</code> to execute constracted query on Athena, <code>exec()</code> returns a dataframe.</p>"},{"location":"athenaSQL/queries/#supported-queries","title":"Supported queries","text":"<ul> <li>SELECT</li> <li>INSERT INTO</li> <li>CREATE</li> <li>CREATE AS</li> <li>CTE</li> <li>UNLOAD</li> <li>WINDOW</li> </ul>"},{"location":"athenaSQL/queries/create/","title":"CREATE","text":"<p>Synopsis</p> <pre><code>table.create()\n    [.ifNotExists(bool)]\n    [.columns(new_column [,...])]\n    [.partitionBy(column [,...])]\n    [.clusterBy(column [,...])]\n    [.location(location)]\n    [.row_format(row_format)]\n    [.stored_as(file_format)]\n    [.stored_as_io(input_format, output_format)]\n    [.serde_properties({\"property\":\"value\" [,...]})]\n    [.tbl_properties({\"property\":\"value\" [,...]})]\n    [.exec()]\n</code></pre> <p>Create an external table. Unlike <code>CREATE AS</code> table schema and properties must be specified.</p> <p>The column type passed when creating a table is special type of column( <code>NewColumn</code> ) that accepts column name and data type. NewColumn can be created using <code>nCol()</code> function.</p> <p><code>row_format()</code> only supports SERDE. ROW FORMAT DELIMITED is not supported. Since DELIMITED is not supported properties have to be specified through <code>serde_properties()</code>. More ref</p> <p><code>stored_as_io()</code> is an alternative for <code>stored_as()</code> that accept input and output format. Both methods cannot be called on a single instance simultaneously. <code>stored_as()</code> accept single row format. These row formats can be imported from <code>athenaSQL.column_type.COLUMNTYPE</code>. Supported row formats: SEQUENCEFILE, TEXTFILE, RCFILE, ORC, PARQUET, AVRO, ION. By default TEXTFILE is selected.</p> <p>Using CREATE</p> <pre><code>from athenaSQL.functions import F\nfrom athenaSQL.column_type import COLUMNTYPE\n\n# create abstract representation of the table\nnew_table = AthenaTable(database='db_name', table='new_table')\n\ncreate_table_query = (new_table.create()\n                         .columns(\n                             nCol('col1', COLUMNTYPE.string),\n                             nCol('col2', COLUMNTYPE.char(4)),\n                             nCol('col3', COLUMNTYPE.int))\n                         .partitionBy('col3')\n                         .clusterBy('col2', buckets=4)\n                         .row_format('org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe')\n                         .stored_as_io(\n                             input_format='org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n                             output_format='org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat')\n                         .location('s3://S3-bucket-location')\n                         .tbl_properties({\n                             \"bucketing_format\":\"spark\",\n                             \"parquet.compression\":\"SNAPPY\"}))\n\ncreate_table_query.show_query()\n</code></pre> <pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS\n\"db_name\".\"new_table\" (\n    `col1` string,\n    `col2` char(4)\n)\nPARTITIONED BY (col1,col2)\nCLUSTERED BY (col2) INTO 4 BUCKETS\nROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\nOUTPUTFORMAT\n    'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nLOCATION 's3://S3-bucket-location/'\nTBLPROPERTIES (\n    \"bucketing_format\"=\"spark\",\n    \"parquet.compression\"=\"SNAPPY\"\n)\n</code></pre>"},{"location":"athenaSQL/queries/create_as/","title":"CREATE AS","text":"<p>Synopsis</p> <pre><code>table.createAs(select_query)\n    [.withData(bool)]\n    [.withTblProps({\"property\":\"value\" [,...]})]\n</code></pre> <p>Create a table from a dataset that is a result of a select query. You can use <code>.withTblProps()</code> to specify table properties. CREATE AS Doc</p> <p>Using CREATE AS</p> <pre><code># let's use select_salary query from SELECT query section\n\nsalary_table = Athena('employee_db').table('salary')\n\ncreate_salary_table = salary_table.createAs(select_salary).withTblProps({\n                            \"format\":\"AVRO\",\n                            \"is_external\":False,\n                            \"vacuum_max_snapshot_age_ms\":259200,\n                            \"partitioning\":['country', 'city']})\n\ncreate_salary_table.show_query()\n</code></pre> <pre><code>CREATE TABLE \"employee_db\".\"salary\"\nWITH (\n    format='AVRO',\n    is_external=false,\n    vacuum_max_snapshot_age_ms=259200,\n    partitioning=ARRAY['country', 'city']\n    )\nAS\nSELECT\n    city,\n    AVG(age) AS age_avg,\n    AVG(salary) AS salary_avg,\n    MIN(salary) AS salary_min,\n    MAX(salary) AS salary_max,\n    MIN(country) AS country\nFROM \"db_name\".\"table_name\"\nWHERE\n    age &gt; 10 AND\n    country = 'Ethiopia'\nGROUP BY city\nHAVING\n    AVG(salary) &gt; 15000\nORDER BY salary_avg DESC\n</code></pre>"},{"location":"athenaSQL/queries/cte/","title":"CTE","text":"<p>Synopsis</p> <pre><code>withTable(cte_name, select_query)\n    [.withTable(cte_name, (table|temp)select_query) [,...]]\n    .table(table_name)\n    .select([column, ...])\n    [.filter(condition [,...])]\n    [.groupBy(column [,...])]\n    [.filterGroup(condition [,...])]\n    [.agg(aggregation [,...])]\n    [.limit(limit)]\n    [.orderBy(column [,...])]\n    [.exec()]\n</code></pre> <p>CTE is a type of select query that let's you create a temporary tables instead of using sub-queries. CTE can be used as an input where ever a select query can.</p> <p>Temporary tables are created using <code>.withTable()</code> method, it accept name for temporary table and a select query. The first select query that is passed to CTE must be a select statement on a table. Subsucent CTEs can be constracted from a select statement on a table or a temporary table(another CTE table).</p> <p>CTE is a type of select query thus it should be finished with a select statment. After creating the temporary tables a table name should be selected for the final select statement. Selecting a table name returns an abstract table for the selected table name with only <code>.select()</code> query capability.</p> <p>Using CTE</p> <pre><code>from athenaSQL import TempTable\nfrom athenaSQL.queries import withTable\nfrom athenaSQL.functions import col\n\ntable_demo = AthenaTable(database='db_name', table='demo')\nselect_demo = table_demo.select('demo_id', 'demo_name', 'demo_addr')\n\nselect_demo_in_addis = (TempTable('demos')\n                            .select('demo_id', 'demo_name', 'demo_addr')\n                            .filter(col('demo_addr') == 'addis'))\n\ncta_query = (withTable('demos', select_demo)\n                .withTable('demo_in_addis', select_demo_in_addis)\n                .table('demo_in_addis')\n                .select())\n\ncta_query.show_query()\n</code></pre> <pre><code>WITH demos AS (\n    SELECT\n        demo_id,\n        demo_name,\n        demo_addr\n    FROM \"db_name\".\"demo\"\n), demo_in_addis AS (\n    SELECT\n        demo_id,\n        demo_name,\n        demo_addr\n    FROM \"demos\"\n    WHERE\n        demo_addr = 'addis'\n)\nSELECT\n    *\nFROM \"demo_in_addis\"\n</code></pre>"},{"location":"athenaSQL/queries/insert_into/","title":"INSERT INTO","text":"<p>Synopsis <pre><code>table.insert(select_query)\n[.column_order(column, [,...])]\n</code></pre></p> <p>Insert query accepts a select query instance to insert selected dataset into a table.</p> <p><code>.column_order()</code> is an optional method to specify and order what column from the dataset to insert.</p> <p>Using INSERT INTO</p> <pre><code># the select_salary select query is used as a dataset from above\n\n# constract salary table from employee_db database\nsalary_table = Athena('employee_db').table('salary')\n\n# constract insert salary dataset query\ninsert_salary_query = salary_table.insert(select_salary)\n\ninsert_salary_query.show_query()\n</code></pre> <pre><code>INSERT INTO \"employee_db\".\"salary\"\nSELECT\n    city,\n    AVG(age) AS age_avg,\n    AVG(salary) AS salary_avg,\n    MIN(salary) AS salary_min,\n    MAX(salary) AS salary_max,\n    MIN(country) AS country\nFROM \"db_name\".\"table_name\"\nWHERE\n    age &gt; 10 AND\n    country = 'Ethiopia'\nGROUP BY city\nHAVING\n    AVG(salary) &gt; 15000\nORDER BY salary_avg DESC\n</code></pre>"},{"location":"athenaSQL/queries/insert_into/#ordered-column-insert","title":"Ordered Column Insert","text":"<pre><code># specified order column to insert from salary dataset\ninsert_salary_query_ordered = (salary_table.insert(select_salary)\n                                .column_order('country', 'city', 'age_avg',\n                                              'salary_avg'))    \n\ninsert_salary_query_ordered.show_query()\n</code></pre> <pre><code>INSERT INTO \"employee_db\".\"salary\"(\n        country,\n        city,\n        age_avg,\n        salary_avg\n    )\nSELECT\n    city,\n    AVG(age) AS age_avg,\n    AVG(salary) AS salary_avg,\n    MIN(salary) AS salary_min,\n    MAX(salary) AS salary_max,\n    MIN(country) AS country\nFROM \"db_name\".\"table_name\"\nWHERE\n    age &gt; 10 AND\n    country = 'Ethiopia'\nGROUP BY city\nHAVING\n    AVG(salary) &gt; 15000\nORDER BY salary_avg DESC\n</code></pre>"},{"location":"athenaSQL/queries/select/","title":"SELECT","text":"<p>Synopsis <pre><code>table.select([column, ...])\n    [.filter(condition [,...])]\n    [.groupBy(column [,...])]\n    [.filterGroup(condition [,...])]\n    [.agg(aggregation [,...])]\n    [.limit(limit)]\n    [.orderBy(column [,...])]\n    [.exec()]\n</code></pre></p> <p>If no column is provided all columns will selected. <code>.filter()</code>, <code>.filterGroup()</code>, <code>.agg()</code> can be chained more than once.</p> <p>A column can be represent by a string. To perform arithmetic, comparison and logical operation on a column, a Column type must be use. To read more on column refer to Column section</p> <p>Creating a column instance</p> <pre><code>import athenaSQL.functions as F\n\n# create a new column instance. we can use F.col or F.column\ncol = F.col('col_name')\ncol2 = F.column('col_2')\n\n# we can perform arithmeti, comparison, logical operation on columns\ncol_multi = col * 10\ncol_compare = col &gt;= 10\ncol_logical = (col &gt;= 10 and col2 &lt;= 15)\n</code></pre>"},{"location":"athenaSQL/queries/select/#select-chain-method-conditions","title":"SELECT Chain Method Conditions","text":"<ul> <li><code>.filterGroup()</code> can only be applied to grouped select query.</li> <li>If no column is provided on a grouped selecte query only grouping     columns will be selected.</li> <li>In grouped select query non-grouping columns must be aggregated.</li> <li>Aggregating already selected column will replace it unless once of     them is aliased.</li> <li>Alternative to using <code>.agg()</code> is to directly add aggregated columns     to <code>.select()</code>.</li> </ul> <p>Using SELECT Query</p> <pre><code># let's use the table instance from `AthenaTable` example\ntable.select().show_query()\n</code></pre> <pre><code>SELECT\n    *\nFROM \"db_name\".\"table_name\"\n</code></pre>"},{"location":"athenaSQL/queries/select/#comprehensive-select-query","title":"Comprehensive SELECT Query","text":"<pre><code>import athenaSQL.functions as F\nfrom athenaSQL.orders import ASC, DESC\n\nselect_salary = (table.select('city', 'country')\n                    .filter(F.col('age') &gt; 10)\n                    .filter(F.col('country') == 'Togo')\n                    .groupBy('city')\n                    .filterGroup(F.avg(F.col('salary')) &gt; 15000)\n                    .agg(F.avg(F.col('age')).alias('age_avg'))\n                    .agg(F.avg(F.col('salary')).alias('salary_avg'))\n                    .agg(F.min(F.col('salary')).alias('salary_min'))\n                    .agg(F.max(F.col('salary')).alias('salary_max'))\n                    .agg(F.min(F.col('country')).alias('country'))\n                    .orderBy(ASC('salary_avg')))\n\nselect_salary.show_query()\n</code></pre> <pre><code>SELECT\n    city,\n    AVG(age) AS age_avg,\n    AVG(salary) AS salary_avg,\n    MIN(salary) AS salary_min,\n    MAX(salary) AS salary_max,\n    MIN(country) AS country\nFROM \"db_name\".\"table_name\"\nWHERE\n    age &gt; 10 AND\n    country = 'Togo'\nGROUP BY city\nHAVING\n    AVG(salary) &gt; 15000\nORDER BY salary_avg DESC\n</code></pre>"},{"location":"athenaSQL/queries/unload/","title":"UNLOAD","text":"<p>Synopsis</p> <pre><code>unload(select_query)\n    .location(location)\n    [.withTblProps({\"property\":\"value\" [,...]})]\n</code></pre> <p><code>UNLOAD</code> lets you dump a select query result to specified location. This query is not directly callable on table instance, it is a wrapper query for <code>SELECT</code> query. Doc</p> <p>Using UNLOAD</p> <pre><code>from athenaSQL.queries import unload\n\n# we can use the cta_query as an input\nunload_query = (unload(cta_query)\n                    .location('s3://S3-bucket-name/sub-location/')\n                    .withTblProps({\"format\":\"PARQUET\", \"compression\":\"gzip\"}))\n\nunload_query.show_query()\n</code></pre> <pre><code>UNLOAD (\n    WITH demos AS (\n        SELECT\n            demo_id,\n            demo_name,\n            demo_addr\n        FROM \"db_name\".\"demo\"\n    ), demo_in_addis AS (\n        SELECT\n            demo_id,\n            demo_name,\n            demo_addr\n        FROM \"demos\"\n        WHERE\n            demo_addr = 'addis'\n    )\n    SELECT\n        *\n    FROM \"demo_in_addis\"\n)\nTO 's3://S3-bucket-name/sub-location/'\nWITH (\n    format='PARQUET',\n    compression='gzip'\n    )\n</code></pre>"},{"location":"athenaSQL/queries/window/","title":"WINDOW","text":"<p>Synopsis</p> <pre><code>window_function([operand, [offset, [default]]])\n    .over(Window\n            .partitionBy(column, [,...])\n            [.orderBy(Column, [,...])]\n    )\n    [.alias(alias)]\n</code></pre> <p>Window functions are imported and called as a python function. The <code>over</code> clause is passed as a Window query. Aggregation function can be used in place of window functions. Lets consider the below window function.</p> <p><code>NTILE(4) OVER (PARTITION BY campaignId ORDER BY flight) AS quartile</code></p> <p><code>NTILE</code> is the function everything after the <code>OVER</code> clause is Window query.</p> <pre><code>from athenaSQL.queries.window import Window\nfrom athenaSQL.functions.window import ntile\n\nwindow_col = ntile(4).over(\n                Window\n                    .partitionBy('campaignId')\n                    .orderBy('flight')\n            ).alias('quartile')\n\nprint(window_col._sql_clause)\n</code></pre> <pre><code>NTILE(4) OVER (PARTITION BY campaignId ORDER BY flight) AS quartile\n</code></pre> <p>Using WINDOW</p> <pre><code>from athenaSQL.queries.window import Window\nfrom athenaSQL.functions.window import row_number\nimport athenaSQL.functions AS F\n\n# lets use salary table\nsum_window = Window.partitionBy('city', 'country').orderBy('salary_avg')\nrow_window = Window.orderBy('salary_avg')\n\nselect_win_query = salary_table.select(\n                        'country',\n                        'city',\n                        'salary',\n                        row_number().over(row_window).alias('row_number'),\n                        F.sum(F.col('salary')).over(sum_window).alias('cumulative_salary')\n                    )\n\nselect_win_query.show_query()\n</code></pre> <pre><code>SELECT\n    country,\n    city,\n    salary,\n    row_number() OVER (\n        ORDER BY salary_avg) AS row_number,\n    SUM(salary) OVER (PARTITION BY city, country\n        ORDER BY salary_avg) AS cumulative_salary\nFROM \"employee_db\".\"salary\"\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/05/30/database-migration-in-sqlalchemy/","title":"Database Migration in SQLAlchemy","text":"<p>Database migration is basically keeping any changes you want to make to your database into code and the code that undo the changes you made. This will help you to revert changes you made to the database anytime you want and more over if you have testing and production environment deploying database changes into production is done by just running a script.</p> <p>Note</p> <p>This is a short explanation of how to setup database migration in SQLAlchemy, if you don't know the basics of database migration, there are plenty of resources online. Checkout your chosing and get back.</p> <p>We are goint to use alembic to manage our database migration.</p>"},{"location":"blog/2024/05/30/database-migration-in-sqlalchemy/#project-setup","title":"Project setup","text":"<p>In SQLAlchemy we have 3 components we should care about when structuring our project, <code>engine</code>, <code>declarative_base</code>, and <code>models</code>. For modularity I like to separate <code>engine</code> into a module but I usually merge <code>declarative_base</code> and <code>models</code> into a subpackage.</p> <p>There are two way to setup your SQLAlchemy project.  - Define <code>declarative_base</code> inside model module. - Have a separate module to store <code>declarative_base</code></p> <p>If you go with the first one, all models and the <code>declarative_base</code> are define in a separe module and alembic imports <code>declarative_base</code> from this module. </p> <p>&lt;--folder structure--&gt;</p> <p>On the second one <code>declarative_base</code> is defined in its own module and for alembic to trace the models they have to be imported with the <code>declarative_base</code>, instead of doing that in alembic I prefer doing it in the database module and from alembic import <code>declarative_base</code> from the database module.</p> <p>&lt;--folder structure--&gt;</p>"},{"location":"blog/2024/05/30/database-migration-in-sqlalchemy/#alembic-setup","title":"Alembic setup","text":"<p>Now we have the SQLAlchemy project setup, let's add database migration. After installing alembic use <code>init</code> command to create the migration at the application level (along side the source code). </p> <pre><code># alembic init &lt;directory&gt;\nalembic init migrations\n</code></pre> <p>env.py the file alembic run. This file is used to link declarive base and the engine with alembic.  alembic.ini contains configuration alembic use when it run. This contains values like database url,  location where migration scripts are stored. versions/ directory where migration scripts are stored. This can be manually changed to different name.</p>"},{"location":"blog/2024/05/30/database-migration-in-sqlalchemy/#lets-do-some-migration","title":"Let's do some migration","text":"<p>Let's create a new model hero and generate a migration script for it. </p> <pre><code># models/hero.py\nfrom sqlalchemy import Column, Integer, String\nfrom src.models.base import Base\n\nclass Hero(Base):\n    __tablename__ = 'heros'\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String)\n    power = Column(String,)\n</code></pre> <p>having multiple database</p>"},{"location":"blog/2024/01/15/notes-big-data-analysis-in-sql/","title":"Notes: Big Data Analysis in SQL","text":"<p>This note consists of fundamental of data, fundamental of SQL, traditional databases, big data and distributed cloud storages and query engines.</p> <p>Note</p> <p>This is my personal note of Modern Big Data Analysis with SQL Specialization. This is only intended as a personal reference.</p>"},{"location":"blog/2024/01/15/notes-big-data-analysis-in-sql/#fundamentals-of-data","title":"Fundamentals of Data","text":"<p>Data is a representation of something that capture some features and ignore others. What this means is that there is a selective representation based on a context, purpose, complexity, managabiluty, etc... </p> <ul> <li>Analog Data: a continuous representation of something. It has infinite values. At any given instance or point, it has a unique continuous value.</li> <li>Digital Data: a discrete representation of something. It has finite values. It is a sample representation of analog data.</li> </ul> <p>For instance, a mechanical clock has a unique value at any given instance but a digital clock has a finite values that represent a section of time duration like 1 sec to 2 sec but analog clock has infinite values between 1 sec and 2 sec.</p> <p>Data is store unorganized in a data store. In order to make sense of the data and easily retrieved, data is organized. Databases are used to store organized data.</p>"},{"location":"blog/2024/01/15/notes-big-data-analysis-in-sql/#dbms","title":"DBMS","text":"<ul> <li> <p>Is a software that lets you organize and manage your data. It gives you an interface to store and retrieve data to and from a database and share your data.</p> </li> <li> <p>A DBMS should have at list these four functionalities</p> <ul> <li>Design how your data is stored in the database. Different data sources will be separated and have their own features.</li> <li>Update/Add records into the database.</li> <li>Retrieve data from the database base to answer various questions.</li> <li>Control access/Manage your data. You need to be able to create accounts and manage who has access to a particular data.</li> </ul> </li> </ul>"},{"location":"blog/2024/01/15/notes-big-data-analysis-in-sql/#rdbms","title":"RDBMS","text":"<ul> <li> <p>In 1970 F.E CODD's paper on relational models laid the foundation for relational databases.</p> </li> <li> <p>In 1974 IBM created SQL to interact with relational databases. It was originally called SEQUEL hence the ambiguous pronunciation.</p> </li> <li> <p>In 2000s the big data era start the rise of NoSQL for non-relational datasets. For structured data relational databases are still in high demand.</p> </li> </ul> <p>In RDBMS data is being stored in a table, collection of tables is a database. SQL is used to interact with the database. Based on the four functionalities SQL commands are categorized into 4.</p> <p>Data Definition Language(DDL)</p> <ul> <li>CREATE - create a table</li> <li>ALTER - change table property</li> <li>DROP - remove a table</li> </ul> <p>Data Manipulation Language(DML)</p> <ul> <li>INSERT - add record</li> <li>UPDATE - update record</li> <li>DELETE - remove record</li> </ul> <p>Data Query Language(DQL) Or Query</p> <ul> <li>SELECT - retrieve data</li> </ul> <p>Data Control Language(DCL)</p> <ul> <li>GRANT - give data access control</li> <li>REVOKE - take away access control</li> </ul> <p>Apart from this 4 major categories there are other SQL commands. Since SQL is constantly evolving there are other commands that are not in the standard sql. Due to various reasons like competition, different RDBMS have their own SQL dialect. Although for the most part they are alike.</p> <p>SQL has gain a huge acceptance even non-relational data/big data management systems and query engines use it as their primary language to interact with data. System like Hive, Impala and Athena.</p>"},{"location":"blog/2024/01/15/notes-big-data-analysis-in-sql/#operational-analytical-databases","title":"Operational &amp; analytical databases","text":"<p>Operational databases: designed to answer what is current state of a process. Optimized for fast read and write of the on going state.</p> <p>Analytical databases: designed to answer analytical queries on a stable large historical data. Optimized for read heavy operation and complex queries.</p> <p>Operational data is bulk loaded into analytical databases periodically.</p> <p>Well most of the thing is pretty familiar here and no need to take notes. I will just add whatever I want to remember.</p> <p>DECIMAL(n, d) - a decimal value with n number of digits and d number of those digits are to the right of the decimal point. DECIMAL(5, 2) is BETWEEN -999.99 AND 999.99.</p> <p>BLOB(Binary Large Object), store up to 4 gb binary data. Movies, media files\u2026</p> <p>CLOB(Character Large Object), store up to 4 gb character data. Books, HTML pages\u2026</p> <p>They are not supported in most of RDBMS. Mostly they are stored separately as files.</p> A STAR Scenario <p>Having multiple foreign keys as a compound primary key.</p> <p></p> <p>A ride is uniquely identified as a combination of rider, driver, time_of_day, type_of_service and traffic_condition ids. This is smart. And this is a star schema. This way of putting it gives a different perspective of the star schema I know from big data realm.</p>"},{"location":"blog/2024/01/15/notes-big-data-analysis-in-sql/#normalization","title":"Normalization","text":"<p>3NF</p> <ol> <li>Primary key - every table should have a primary key. Avoid intelligent keys - use a key that doesn\u2019t have any meaning about the item.</li> <li>Atomic columns - a column value should be a single value. It should not contain combination of values. In other word you should be able to manipulate the value inside a column with out loading it to memory.</li> <li>No repeating groups - a column should not contain multiple records in a single row as repeating values. The repeating values should be stored in a separate table with foreign keys.</li> <li>Non-key columns describe only the whole key -  non-key columns only describe about the primary key not about other non-key columns. If a non-key column describes about another non-key column, they should be stored in a separate table and referenced with a foreign key.</li> <li>No derived columns - a column should not be a computed value of other columns. This is redundant and if one of the columns are changed the computed value would be incorrect.</li> </ol>"},{"location":"blog/2024/01/15/notes-big-data-analysis-in-sql/#denormalization","title":"Denormalization","text":"<p>If at least one of the principles are intentionally violated we get denormalized denormalized. This doesn\u2019t mean designing a database with out considering the normalization rules. To denormalize a best practice is to first normalize it and intentionally violate the normalize rule to achieve some thing like fast read.</p> <p>Normalization advantages</p> <ul> <li>Limit data anomalies: different entities are stored in a separate tables thus<ul> <li>Adding new entity will not affect other entities</li> <li>Updating and deleting of an entity is done at on place and will take effect everywhere.</li> </ul> </li> <li>Enforce data structure:<ul> <li>Prevent you from duplicating records</li> <li>Dependencies should be fulfilled before adding or deleting a record. This dependenceis are created with table relations.</li> </ul> </li> <li>Size: since it prevents data duplication the size is much smaller.</li> </ul> <p>Denormalization advantages</p> <ul> <li>SELECT speed: by reducing join operation and on query computations your SELECT query will be much faster.</li> </ul> <p>Denormalization is suited for analytics databases since it is suitable for complex queries and faster SELECT speed on larger data size. </p> <p>Normalization is suited for operational databases since size is relatively small and mostly a lookup queries are performed. Normalization rules will help you to keep your data integrity.</p> <p>Nothing but the key</p> <p>A non-key column should provide a fact about the key, the whole key, and nothing but the key, so help me CODD.</p>"},{"location":"blog/2024/02/19/note-grokking-algorithms/","title":"Note: Grokking Algorithms","text":"<p>This is my note for the book grokking algorithms. It is really a nice book. It gives most of the fundamental concepts in simple terms. My number one recommendation for any one who is starting data stractures and algorithms.</p>"},{"location":"blog/2024/02/19/note-grokking-algorithms/#big-o-notation","title":"Big O notation","text":"<p>A notation that tells you how fast an algorithm run. It doesn\u2019t tell you how long it will take but it gives you the number of operations it will need to perform for the worst case scenario. This is particularly useful when number of items increase. </p> <p>Common Big O run times</p> <ul> <li><code>O(1)</code> constant time - Reading array</li> <li><code>O(log n)</code> log time - Binary search</li> <li><code>O(n)</code> linear time  - Simple search</li> <li><code>O(n * log n)</code> -</li> <li><code>O(n\u00b2)</code> Quadratic time - selection sort</li> <li><code>O(n!)</code> factorial time - The traveling sales person</li> </ul> <p>Algorithm speed isn\u2019t measured in seconds, but in growth of the number of operations. Instead, we talk about how quickly the run time of an algorithm increases as the size of the input increases.</p> <p><code>O(log n)</code> is faster than<code>O(n)</code>, but it gets a lot faster as the list of items you are searching grows. </p>"},{"location":"blog/2024/02/19/note-grokking-algorithms/#binary-search","title":"Binary Search","text":"<p>Input: a sorted list of items</p> <p>Big O: <code>O(log n)</code></p> <p>Return: position of item else NULL</p> <p>Steps</p> <ul> <li>Check if the middle items is what you are looking for<ul> <li>If target is greater than middle look into items to the right of the middle value</li> <li>if target is less than the middle value look into items to the left of the middle value</li> </ul> </li> <li>Do the above operation when looking for a target in any subset of the items until target is found or run out of items to look into.</li> </ul> Binary Search Pseudocode <pre><code>SET items to list of sorted items\nSET low to zero\nSET high to number of items - 1  // if zero based indexing\n\nINPUT target\n\nWHILE low &lt;= high\n\n    SET mid to (low + high) / 2 rounded down\n\n    IF item at position mid equals target THEN\n        OUTPUT mid\n    ELSE IF item at position mid is less than target THEN\n        SET high to mid - 1\n    ELSE\n        SET low to mid + 1\n    ENDIF\n\nENDWHILE\n\nOUTPUT NULL  // return null if target not in list\n</code></pre>"},{"location":"blog/2024/02/19/note-grokking-algorithms/#array-vs-linked-list","title":"Array vs Linked List","text":"Array Linked List finite items consecutively in memory infinite items randomly in memory, item address linked to previous item Stores finite items in a memory next to each other Stores infinite items in a memory randomly Once created, additional items can not be added Additional items can be added after creation It is <code>O(1)</code> to access an item, <code>O(n)</code> to insert an item &amp; <code>O(n)</code> to delete an item It is <code>O(n)</code> to access an item, <code>O(1)</code> to insert an item &amp; <code>O(1)</code> to delete an item Suitable for lots of reads and few inserts We assume the position to insert into and deletion from is known Random access, this makes array the most used Sequential access All items should be the same type Items can be different types"},{"location":"blog/2024/02/19/note-grokking-algorithms/#selection-sort","title":"Selection sort","text":"<p>Input: a list of items</p> <p>Big O: <code>O(n\u00b2)</code></p> <p>Return: sorted items</p> <p>Steps</p> <ul> <li>Go through the list of items and take out the smallest value</li> <li>Repeat this until all items have been sorted.</li> </ul> <p>Call Stack</p> <p>The stack is a data structure that which the first data pulled from the stack is the last data pushed to it. It is used in the computer memory to execute programs. This  stack is called call stack. Whenever a function is called it is pushed to the call stack and when the function returns something it is popped out of the stack.</p>"},{"location":"blog/2024/02/06/notes-zero-etl/","title":"Notes: Zero-ETL","text":"<p>At the time of writing Zero ETL is an integration tool that makes transactional data available in Redshift in near real time. This is useful if real time analytical data in Redshift is necessary. It support Aurora and RDS MySQL as data source.</p>"},{"location":"blog/2024/02/06/notes-zero-etl/#what-problem-zero-etl-solve","title":"What problem zero ETL solve?","text":"<ul> <li>Remove the complexity of ETL management, monitoring and maintenance</li> <li>Low latency data integration. Data is updated imidiately when the source is updated. E.g when RDBMS updated, Redshift will get updated updated right after.</li> <li>It is particularly useful if real time data is necessary</li> </ul>"},{"location":"blog/2024/02/06/notes-zero-etl/#disadvantage-of-zero-etl","title":"Disadvantage of zero ETL","text":"<ul> <li>There is no stagings, which means data is highly transformed. Which means you will most likely will end up creating a new zero ETL for new scenarios.</li> <li>For none real time data, I don\u2019t see the advantage</li> </ul>"},{"location":"cookbooks/","title":"Cookbooks","text":""},{"location":"cookbooks/bash/","title":"Bash Cookbook","text":""},{"location":"cookbooks/bash/#delete-filesdirectory-recursively-from-s3","title":"Delete files/directory recursively from S3","text":"<pre><code>aws s3 ls BUCKET-NAME --recursive &gt; file_Structure\n# after all file keys are extracted filter only the target dir to delete\n\n# create objects of 1000 file key and delete\ncat file_Structure | xargs -P8 -n1000 bash -c 'aws s3api delete-objects \n--bucket ml-box-data \n--delete \"Objects=[$(printf \"{Key=%s},\" \"$@\")], Quiet=true\"' _\n</code></pre>"},{"location":"cookbooks/bash/#delete-filesdirectory-recursively","title":"Delete files/directory recursively","text":"<p>Remove <code>-v</code> flag if you don't want verbose output.</p> <pre><code># find and list all files that match *.bak pattern\nfind . -name \"*.bak\" -type f         \n\n# find and delete all files that match *.bak pattern\nfind . -name \"*.bak\" -type f -delete \n\nfind . -name \"*.egg-info\" -type d # find all dir that match the pattern\nfind . -name \"*.egg-info\" -type d -exec rm -rv {} + #(1)!\nfind . -name \"*.egg-info\" -type d -exec rm -rv {} \\; #(2)!\n</code></pre> <ol> <li> <p>Find and delete all dir that match the pattern.     The <code>+</code> at the end will result in <code>rm -rv file1 file2 ...</code></p> </li> <li> <p>Find and delete all dir that match the pattern.     The <code>\\;</code> at the end will result in <code>rm -rv file1;</code> <code>rm -rv file2;</code> ...</p> </li> </ol>"},{"location":"cookbooks/bash/#difference-between-and","title":"Difference between <code>$()</code> and <code>${}</code>","text":"<p><code>$()</code> means first evaluate this and then evaluate the line.</p> <pre><code>echo $(pwd)/myFile.txt\n# interpreted as\necho /my/path/myFile.txt\n</code></pre> <p><code>${}</code> expands a variable.</p> <pre><code>MY_VAR=toto\necho ${MY_VAR}/myFile.txt\n# interpreted as\necho toto/myFile.txt\n</code></pre>"},{"location":"cookbooks/bash/#download-all-files-in-a-directory-using-wget","title":"Download all files in a directory using wget","text":"<p>Pass the\u00a0<code>-np</code> / <code>--no-parent</code>, <code>-r</code> / <code>--recursive</code> and <code>-R</code> / <code>--reject</code> options to\u00a0wget - stackoverflow</p> <pre><code>wget --recursive --no-parent -R \"index.html*\" http://example.com/path/to/files/\n\nwget -r -np -R \"index.html*\" http://example.com/path/to/files/\n</code></pre>"},{"location":"cookbooks/javascript/","title":"Javascript Cookbook","text":""},{"location":"cookbooks/javascript/#extract-data-from-html-table","title":"Extract data from html table","text":"<p>Stackoverflow Link</p>"},{"location":"cookbooks/python/","title":"Python Cookbook","text":""},{"location":"cookbooks/python/#handling-unnamed-n-column-in-pandas","title":"Handling <code>Unnamed: n</code> column in pandas","text":"<ul> <li>Set index to False when writing to csv file.</li> <li>Set col index 0 when reading csv file.</li> </ul> <pre><code>pd.to_csv('flat_file.csv', index=False)   # set index to false when writing\npd.read_csv('flat_file.csv', index_col=0) # set index_col to 0 when reading\n</code></pre>"},{"location":"cookbooks/python/#change-dtype-in-pandas","title":"Change dtype in pandas","text":"<ul> <li>Types can be given at reading</li> <li>After reading column type can be changed with <code>to_numeric()</code> <code>astype()</code> <code>to_datetime()...</code></li> </ul> <p><code>to_numeric()</code> will change type to integer or float as appropriate. <code>astype()</code> let you specify the type you want. It accept a single type or dict of column names paired with types.</p> <p>Reference</p> <pre><code># Give column type when reading, setting dtypes of columns on dtype\ndf = pd.read_csv('flat_file.csv', dtype={'col_name': str}) \n\n# Changing types of dataframe column\n\n# change col_name type to numeric. \ndf.col_name = pd.to_numeric(df.col_name) \n# change col types with astype\ndf = df.astype({'col_name': str, 'col_name_2': float}) \n# changing single col\ndf.col_name = df.col_name.astype(float) \n</code></pre>"},{"location":"the-universe/","title":"The universe ---","text":""},{"location":"the-universe/our-kids-and-them-kids/","title":"Our kids and them kids","text":"*22/10/2021 - in Seychelles anxiously waiting for the plane at the gates*   <p> The family of four we got at the airport at Seychelles that missed their flight like us. They got two daughters, roughly age 10 &amp; 13. The way they dress looks fine for a flight to me. Both of them have pink beats headphones wrapping around their necks. They seem confident, you know they are from financially successful family and they brought them to Seychelles, what is there to be more confident about. But the way they manifest their confidence looks more artificial and dependent on the wealth they think their family has. Anything they want their family will provide. They are above their peers. Nothing is stopping them from being confident. It feels more like outward confidence. It is heavily dependent on the stuff they put on themselves and the wealthy their family can show to other people (or peers). They are confident because their attention(people / peers) will see good things on them. Their personality seems to me it can shatter if someone else with bigger wealthy comes in the picture, they lose the wealthy( of course, anybody will be affect by this, degree differs tho) or the people / peers around them loose interest in what they can show off.  <p> On the other side while I was waiting for the on-boarding, I saw some french people I think with 3 kids - son, daughter, son - roughly 14, 8, 6 ages. Before the family came to on board, the elder son comes to the front inform the ticket checker that they are coming. And they way he do it is not bad for a kid his age. Then the family came and have their thickets checked and on board. before they on board the family direct all kids to say bye to the ticket checker and they left. Since the kids are relatively younger it is hard to make comparison but here we go. The kids look confident and independent. The confidence is not from outside-in tho. It is more like inside out. I could feel it. You get their confidence on how they speak, approach people, act, look their surrounding and react to their surrounding. It doesn't feel like their confidence piller on something that could easily wipeable, the foundation is so strong you need to break them apart to shatter their confidence.  <p> This is the kind of confidence we need our children to build. The more stronger the foundation, the harder it is to shatter it. The better the material used to build the foundation, the harder it is to shatter. We need to know what material to use to build the foundation of our kids. Read read read to the dying of the hound."},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/database/","title":"Database","text":""},{"location":"blog/category/note/","title":"Note","text":""},{"location":"blog/category/algorithm/","title":"Algorithm","text":""},{"location":"blog/category/etl/","title":"ETL","text":""},{"location":"blog/category/sql/","title":"SQL","text":""},{"location":"blog/category/big-data/","title":"Big Data","text":""}]}